{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python Cheat Sheet for Data Science.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWAzyhaLE79euPOUht7KWK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayKozaronek/03_Courses/blob/master/Python_Cheat_Sheet_for_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzjeFS_9qxbO",
        "colab_type": "text"
      },
      "source": [
        "# Cheatsheet Data Science \n",
        "- Pandas\n",
        "- Numpy\n",
        "- Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYcv8387oLHm",
        "colab_type": "text"
      },
      "source": [
        "Pandas, Numpy, and Scikit-Learn are among the most popular libraries for data science and analysis with Python.\n",
        "\n",
        "Numpy is used for lower level scientific computation. Pandas is built on top of Numpy and designed for practical data analysis in Python. Scikit-Learn comes with many machine learning models that you can use out of the box.\n",
        "\n",
        "In this cheat sheet, we’ll summarize some of the most common and useful functionality from these libraries. Let’s jump straight in!\n",
        "\n",
        "Source: https://elitedatascience.com/python-cheat-sheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnUwgExpjOK",
        "colab_type": "text"
      },
      "source": [
        "## 1. Importing Data\n",
        "\n",
        "Any kind of data analysis starts with getting hold of some data. Pandas gives you plenty of options for getting data into your Python workbook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDkbin0FpsK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From a CSV file\n",
        "pd.read_csv(filename)\n",
        "\n",
        "#From a delimited text file (like TSV)\n",
        "pd.read_table(filename) \n",
        "\n",
        "# From an Excel file\n",
        "pd.read_excel(filename)\n",
        "\n",
        "# Reads from a SQL table/database\n",
        "pd.read_sql(query, connection_object) \n",
        "\n",
        "# Reads from a JSON formatted string, URL or file\n",
        "pd.read_json(json_string) \n",
        "\n",
        "# Parses an html URL, string or file and extracts tables to a list of dataframes\n",
        "pd.read_html(url)\n",
        "\n",
        "# Takes the contents of your clipboard and passes it to read_table()\n",
        "pd.read_clipboard()\n",
        "\n",
        "# From a dict, keys for column names, values for data as lists \n",
        "pd.DataFrame(dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQkWJ773psa6",
        "colab_type": "text"
      },
      "source": [
        "## 2.Exploring Data\n",
        "Once you have imported your data into a Pandas dataframe, you can use these methods to get a sense of what the data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSDl9y8Tpyh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prints number of rows and columns in dataframe\n",
        "df.shape()\n",
        "\n",
        "# Prints first n rows of the DataFrame\n",
        "df.head(n)\n",
        "\n",
        "# Prints last n rows of the DataFrame\n",
        "df.tail(n)\n",
        "\n",
        "# Index, Datatype and Memory information\n",
        "df.info()\n",
        "\n",
        "# Summary statistics for numerical columns\n",
        "df.describe()\n",
        "\n",
        "# Views unique values and counts\n",
        "s.value_counts(dropna=False)\n",
        "\n",
        "# Unique values and counts for all columns\n",
        "df.apply(pd.Series.value_counts)\n",
        "\n",
        "# Returns the mean of all columns\n",
        "df.mean()\n",
        "\n",
        "# Returns the correlation between columns in a DataFrame\n",
        "df.corr()\n",
        "\n",
        "# Returns the number of non-null values in each DataFrame columns\n",
        "df.count()\n",
        "\n",
        "# Returns the highest value in each column\n",
        "df.max()\n",
        "\n",
        "# Returns the lowest value in each column\n",
        "df.min()\n",
        "\n",
        "# Returns the median of each column\n",
        "df.median()\n",
        "\n",
        "# Returns the standard deviation of each column\n",
        "df.std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-QKflTvpzbm",
        "colab_type": "text"
      },
      "source": [
        "## 3.Selecting\n",
        "Often, you might need to select a single element or a certain subset of the data to inspect it or perform further analysis. These methods will come in handy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiNq0lF7pzrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns column with label col as Series\n",
        "df[col]\n",
        "\n",
        "# Returns Columns as a new DataFrame\n",
        "df[[col1, col2]]\n",
        "\n",
        "# Selection by position(selects first element)\n",
        "s.iloc[0]\n",
        "\n",
        "# Selection by index (selects element at index 0)\n",
        "s.loc[0]\n",
        "\n",
        "# First row \n",
        "df.iloc[0,:]\n",
        "\n",
        "# First element of first column\n",
        "df.iloc[0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKgmoLSVpzyj",
        "colab_type": "text"
      },
      "source": [
        "## 4.Data Cleaning\n",
        "If you’re working with real world data, chances are you’ll need to clean it up. These are some helpful methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0iQCTJgpz4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Renames columns\n",
        "df.columns = [\"a\",\"b\",\"c\"]\n",
        "\n",
        "# Checks for null Values, Returns Boolean Array\n",
        "pd.isnull()\n",
        "\n",
        "# Opposite of s.isnull()\n",
        "pd.notnull()\n",
        "\n",
        "# Drops all rows that contain null values \n",
        "df.dropna()\n",
        "\n",
        "# Drops all columns that contain null values \n",
        "df.dropna(axis=1)\n",
        "\n",
        "# Drops all rows that have less than n non null values \n",
        "df.dropna(axis1, thresh = n)\n",
        "\n",
        "# Replaces all null values with x \n",
        "df.fillna(x)\n",
        "\n",
        "# Replaces all null values with the mean(mean can be replaced with almost any statistical function)\n",
        "df.fillna(s.mean())\n",
        "\n",
        "# Converts the datatype of the series to float\n",
        "s.astype(float)\n",
        "\n",
        "# Replaces all values equal to 1 with \"one\"\n",
        "s.replace(1, \"one\")\n",
        "\n",
        "# Replaces all 1 with \"one\" and 3 with \"three\"\n",
        "s.replace([1,3], [\"one\",\"three\"])\n",
        "\n",
        "# Mass renaming of columns \n",
        "df.rename(columns = lambda x:x+1)\n",
        "\n",
        "# Selective renaming \n",
        "df.rename(columns = {\"old_name\":\"new_name\"})\n",
        "\n",
        "# Changes the index \n",
        "df.set_index(\"column_one\")\n",
        "\n",
        "# Mass renaming of index \n",
        "df.rename(index = labda x:x+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgfzmzCDpz98",
        "colab_type": "text"
      },
      "source": [
        "## 5.Filter, Sort and Group By\n",
        "Methods for filtering, sorting and grouping your data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9gL4zmWp0K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rows where the col column is greater than 0.5\n",
        "df[df[col] >0.5]\n",
        "\n",
        "# Rows where 0.5 < col < 0.7\n",
        "df[(df[col] >0.5]) & (df[col] <0.7])]\n",
        "\n",
        "# Sorts values by col1 in ascending order \n",
        "df.sort_values(col1)\n",
        "\n",
        "# Sorts values by col2 in descending order \n",
        "df.sort_values(col1, ascending = False)\n",
        "\n",
        "# Sorts values by col1 in ascending order, then col2 in descending order\n",
        "df.sort_values([col1,col2], ascending=[True, False])\n",
        "\n",
        "# Returns a groupby object for values from one column\n",
        "df.groupby(col1)\n",
        "\n",
        "# Returns a groupby object with values from multiple columns\n",
        "df.groupby([col1,col2])\n",
        "\n",
        "# Returns the mean of the values in col2, grouped by the values in col1 \n",
        "# (mean can ve replaced with almost any statistical function)\n",
        "df.groupby(col1)[col2].mean()\n",
        "\n",
        "# Creates a pivot table that groups by col1 and calculates the mean of col2 and col3\n",
        "df.pivot_table(index=col1, values = col2,col3, aggfunc=mean)\n",
        "\n",
        "# Finds the average across all columns for every unique column 1 group\n",
        "df.groupby(col1).agg(np.mean)\n",
        "\n",
        "# Applies a function across each column\n",
        "df.apply(np.mean)\n",
        "\n",
        "# Applies a funtion across each row\n",
        "df.apply(np.mean, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ujZn7EVqGFC",
        "colab_type": "text"
      },
      "source": [
        "## 6.Joining and Combining\n",
        "Methods for combining two dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXwSMNjcqIhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adds the rows in df1 to the end of df2 (columns should be identical)\n",
        "df1.append(df2)\n",
        "\n",
        "# Adds the columns in df1 to the end of df2 (rows should be identical)\n",
        "pd.concat([df1,df2], axis =1)\n",
        "\n",
        "# SQL-style joins the columns in df1 with the columns on df2 where the rows for \n",
        "# col have identical values. \n",
        "# How can be \"left\", \"right\", \"outer\", \"inner\"<strong> </strong>\n",
        "df.join(df2, on col1, how=\"inner\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IP3anXrqIxR",
        "colab_type": "text"
      },
      "source": [
        "## 7.Writing Data\n",
        "And finally, when you have produced results with your analysis, there are several ways you can export your data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzZFnA4BqI64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writes to a CSV file \n",
        "df.to_csv(filename)\n",
        "\n",
        "# Writes to an Excel file\n",
        "df.to_excel(filename)\n",
        "\n",
        "# Writes to a SQL table\n",
        "df.to_sql(table_name, connection_object)\n",
        "\n",
        "# Writes to a file in JSON format\n",
        "df.to_json(filename)\n",
        "\n",
        "# Saves as an HTML table\n",
        "df.to_html(filename)\n",
        "\n",
        "# Writes to the clipboard\n",
        "df.to_clipboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ1yYKpcqJBj",
        "colab_type": "text"
      },
      "source": [
        "## 8.Machine Learning\n",
        "The Scikit-Learn library contains useful methods for training and applying machine learning models. \n",
        "\n",
        "For a complete list of the Supervised Learning, Unsupervised Learning, and Dataset Transformation, and Model Evaluation modules in Scikit-Learn, please refer to its user guide.\n",
        "https://scikit-learn.org/stable/user_guide.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADjMZUqZqJI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a043fbd9-5b95-48c2-e133-851f337a1930"
      },
      "source": [
        "# Import libraries and modules \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Load red wine data\n",
        "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "data = pd.read_csv(dataset_url, sep=\";\")\n",
        "\n",
        "# Split data into training and test sets\n",
        "y = data.quality\n",
        "X = data.drop(\"quality\", axis =1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=123,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Declare data preprocessing steps\n",
        "pipeline = make_pipeline(preprocessing.StandardScaler(),\n",
        "                         RandomForestRegressor(n_estimators =100))\n",
        "\n",
        "# Declare hyperparameters to tune \n",
        "hyperparameters = {\"randomforestregressor__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
        "                   \"randomforestregressor__max_depth\": [None, 5,3,1]}\n",
        "\n",
        "# Tune model using cross-validation pipeline\n",
        "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Refit on the entire training set\n",
        "# No additional code needed if clf.refit == True (default is True)\n",
        "\n",
        "# Evaluate model pipeline on test data \n",
        "pred = clf.predict(X_test)\n",
        "print(f\" R2 Score = {r2_score(y_test, pred)}\")\n",
        "print(f\" MSE = {mean_squared_error(y_test, pred)}\")\n",
        "\n",
        "# Save model for future use \n",
        "joblib.dump(clf, \"rf_regressor.pkl\")\n",
        "\n",
        "# To load the model use \n",
        "clf2= joblib.load(\"rf_regressor.pkl\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " R2 Score = 0.4701514619529028\n",
            " MSE = 0.34189718750000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D278iez71Ru-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}